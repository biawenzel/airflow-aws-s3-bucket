[2024-12-12T11:07:37.438-0300] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-12-12T11:07:37.447-0300] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: medallion_architecture.spark_submit manual__2024-12-12T14:07:27.649820+00:00 [queued]>
[2024-12-12T11:07:37.452-0300] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: medallion_architecture.spark_submit manual__2024-12-12T14:07:27.649820+00:00 [queued]>
[2024-12-12T11:07:37.452-0300] {taskinstance.py:2303} INFO - Starting attempt 1 of 1
[2024-12-12T11:07:37.466-0300] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): spark_submit> on 2024-12-12 14:07:27.649820+00:00
[2024-12-12T11:07:37.480-0300] {standard_task_runner.py:63} INFO - Started process 72197 to run task
[2024-12-12T11:07:37.480-0300] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-12-12T11:07:37.483-0300] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'medallion_architecture', 'spark_submit', 'manual__2024-12-12T14:07:27.649820+00:00', '--job-id', '133', '--raw', '--subdir', 'DAGS_FOLDER/dag2.py', '--cfg-path', '/tmp/tmpqrao76h0']
[2024-12-12T11:07:37.484-0300] {standard_task_runner.py:91} INFO - Job 133: Subtask spark_submit
[2024-12-12T11:07:37.511-0300] {task_command.py:426} INFO - Running <TaskInstance: medallion_architecture.spark_submit manual__2024-12-12T14:07:27.649820+00:00 [running]> on host beatriz-linux
[2024-12-12T11:07:37.593-0300] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='medallion_architecture' AIRFLOW_CTX_TASK_ID='spark_submit' AIRFLOW_CTX_EXECUTION_DATE='2024-12-12T14:07:27.649820+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-12T14:07:27.649820+00:00'
[2024-12-12T11:07:37.594-0300] {taskinstance.py:430} INFO - ::endgroup::
[2024-12-12T11:07:37.595-0300] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-12-12T11:07:37.596-0300] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\n    export AWS_ACCESS_KEY_ID=ASIAWAA66KBXXMU7KCLM;\n    export AWS_SECRET_ACCESS_KEY=***;\n    export AWS_SESSION_TOKEN=***;\n    /opt/spark/bin/spark-submit         --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.508         --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain         --conf spark.hadoop.fs.s3a.endpoint=s3.amazonaws.com         --conf spark.hadoop.fs.s3a.region=us-east-1         /home/beatriz/Documentos/airflow/dags/tarefas/tarefas.py']
[2024-12-12T11:07:37.611-0300] {subprocess.py:86} INFO - Output:
[2024-12-12T11:07:39.761-0300] {subprocess.py:93} INFO - 24/12/12 11:07:39 WARN Utils: Your hostname, beatriz-linux resolves to a loopback address: 127.0.1.1; using 192.168.7.5 instead (on interface wlp0s20f3)
[2024-12-12T11:07:39.764-0300] {subprocess.py:93} INFO - 24/12/12 11:07:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-12-12T11:07:40.018-0300] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-12-12T11:07:40.131-0300] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/beatriz/.ivy2/cache
[2024-12-12T11:07:40.131-0300] {subprocess.py:93} INFO - The jars for the packages stored in: /home/beatriz/.ivy2/jars
[2024-12-12T11:07:40.137-0300] {subprocess.py:93} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2024-12-12T11:07:40.138-0300] {subprocess.py:93} INFO - com.amazonaws#aws-java-sdk-bundle added as a dependency
[2024-12-12T11:07:40.139-0300] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-a84f6739-8f61-4efe-a7ec-c6ba6a74df21;1.0
[2024-12-12T11:07:40.139-0300] {subprocess.py:93} INFO - 	confs: [default]
[2024-12-12T11:07:40.378-0300] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2024-12-12T11:07:40.420-0300] {subprocess.py:93} INFO - 	found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2024-12-12T11:07:40.441-0300] {subprocess.py:93} INFO - 	found com.amazonaws#aws-java-sdk-bundle;1.12.508 in central
[2024-12-12T11:07:40.501-0300] {subprocess.py:93} INFO - :: resolution report :: resolve 328ms :: artifacts dl 34ms
[2024-12-12T11:07:40.502-0300] {subprocess.py:93} INFO - 	:: modules in use:
[2024-12-12T11:07:40.502-0300] {subprocess.py:93} INFO - 	com.amazonaws#aws-java-sdk-bundle;1.12.508 from central in [default]
[2024-12-12T11:07:40.502-0300] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2024-12-12T11:07:40.502-0300] {subprocess.py:93} INFO - 	org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2024-12-12T11:07:40.502-0300] {subprocess.py:93} INFO - 	:: evicted modules:
[2024-12-12T11:07:40.503-0300] {subprocess.py:93} INFO - 	com.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.508] in [default]
[2024-12-12T11:07:40.503-0300] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2024-12-12T11:07:40.503-0300] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2024-12-12T11:07:40.503-0300] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-12-12T11:07:40.504-0300] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2024-12-12T11:07:40.504-0300] {subprocess.py:93} INFO - 	|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |
[2024-12-12T11:07:40.504-0300] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2024-12-12T11:07:40.508-0300] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-a84f6739-8f61-4efe-a7ec-c6ba6a74df21
[2024-12-12T11:07:40.508-0300] {subprocess.py:93} INFO - 	confs: [default]
[2024-12-12T11:07:40.516-0300] {subprocess.py:93} INFO - 	0 artifacts copied, 3 already retrieved (0kB/8ms)
[2024-12-12T11:07:42.735-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SparkContext: Running Spark version 3.5.3
[2024-12-12T11:07:42.745-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SparkContext: OS info Linux, 6.8.0-49-generic, amd64
[2024-12-12T11:07:42.745-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SparkContext: Java version 11.0.25
[2024-12-12T11:07:42.775-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO ResourceUtils: ==============================================================
[2024-12-12T11:07:42.775-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-12-12T11:07:42.776-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO ResourceUtils: ==============================================================
[2024-12-12T11:07:42.776-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SparkContext: Submitted application: Medallion Architecture
[2024-12-12T11:07:42.806-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-12-12T11:07:42.822-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO ResourceProfile: Limiting resource is cpu
[2024-12-12T11:07:42.822-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-12-12T11:07:42.901-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SecurityManager: Changing view acls to: beatriz
[2024-12-12T11:07:42.902-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SecurityManager: Changing modify acls to: beatriz
[2024-12-12T11:07:42.903-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SecurityManager: Changing view acls groups to:
[2024-12-12T11:07:42.904-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SecurityManager: Changing modify acls groups to:
[2024-12-12T11:07:42.904-0300] {subprocess.py:93} INFO - 24/12/12 11:07:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: beatriz; groups with view permissions: EMPTY; users with modify permissions: beatriz; groups with modify permissions: EMPTY
[2024-12-12T11:07:43.380-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO Utils: Successfully started service 'sparkDriver' on port 40651.
[2024-12-12T11:07:43.485-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO SparkEnv: Registering MapOutputTracker
[2024-12-12T11:07:43.637-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO SparkEnv: Registering BlockManagerMaster
[2024-12-12T11:07:43.697-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-12-12T11:07:43.698-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-12-12T11:07:43.710-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-12-12T11:07:43.771-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-405d3daf-1285-4cf1-a6e2-a7fd7cef8b31
[2024-12-12T11:07:43.797-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-12-12T11:07:43.832-0300] {subprocess.py:93} INFO - 24/12/12 11:07:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-12-12T11:07:44.279-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-12-12T11:07:44.470-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-12-12T11:07:44.482-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2024-12-12T11:07:44.493-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2024-12-12T11:07:44.605-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO SparkContext: Added JAR /opt/spark/jars/hadoop-aws-3.3.4.jar at spark://192.168.7.5:40651/jars/hadoop-aws-3.3.4.jar with timestamp 1734012462717
[2024-12-12T11:07:44.605-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO SparkContext: Added JAR /opt/spark/jars/aws-java-sdk-bundle-1.12.508.jar at spark://192.168.7.5:40651/jars/aws-java-sdk-bundle-1.12.508.jar with timestamp 1734012462717
[2024-12-12T11:07:44.613-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO SparkContext: Added file file:///home/beatriz/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at file:///home/beatriz/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1734012462717
[2024-12-12T11:07:44.619-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO Utils: Copying /home/beatriz/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2024-12-12T11:07:44.949-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO SparkContext: Added file file:///home/beatriz/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar at file:///home/beatriz/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar with timestamp 1734012462717
[2024-12-12T11:07:44.983-0300] {subprocess.py:93} INFO - 24/12/12 11:07:44 INFO Utils: Copying /home/beatriz/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar
[2024-12-12T11:07:51.392-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO SparkContext: Added file file:///home/beatriz/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at file:///home/beatriz/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1734012462717
[2024-12-12T11:07:51.401-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Utils: Copying /home/beatriz/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2024-12-12T11:07:51.691-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Executor: Starting executor ID driver on host 192.168.7.5
[2024-12-12T11:07:51.708-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Executor: OS info Linux, 6.8.0-49-generic, amd64
[2024-12-12T11:07:51.708-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Executor: Java version 11.0.25
[2024-12-12T11:07:51.709-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-12-12T11:07:51.710-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4593924c for default.
[2024-12-12T11:07:51.728-0300] {subprocess.py:93} INFO - 24/12/12 11:07:51 INFO Executor: Fetching file:///home/beatriz/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar with timestamp 1734012462717
[2024-12-12T11:07:54.828-0300] {subprocess.py:93} INFO - 24/12/12 11:07:54 INFO Utils: /home/beatriz/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar has been previously copied to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/com.amazonaws_aws-java-sdk-bundle-1.12.508.jar
[2024-12-12T11:07:56.600-0300] {subprocess.py:93} INFO - 24/12/12 11:07:56 INFO Executor: Fetching file:///home/beatriz/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1734012462717
[2024-12-12T11:07:56.631-0300] {subprocess.py:93} INFO - 24/12/12 11:07:56 INFO Utils: /home/beatriz/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar has been previously copied to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2024-12-12T11:07:56.993-0300] {subprocess.py:93} INFO - 24/12/12 11:07:56 INFO Executor: Fetching file:///home/beatriz/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1734012462717
[2024-12-12T11:07:57.050-0300] {subprocess.py:93} INFO - 24/12/12 11:07:57 INFO Utils: /home/beatriz/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar has been previously copied to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2024-12-12T11:07:57.309-0300] {subprocess.py:93} INFO - 24/12/12 11:07:57 INFO Executor: Fetching spark://192.168.7.5:40651/jars/aws-java-sdk-bundle-1.12.508.jar with timestamp 1734012462717
[2024-12-12T11:07:57.496-0300] {subprocess.py:93} INFO - 24/12/12 11:07:57 INFO TransportClientFactory: Successfully created connection to /192.168.7.5:40651 after 123 ms (0 ms spent in bootstraps)
[2024-12-12T11:07:57.533-0300] {subprocess.py:93} INFO - 24/12/12 11:07:57 INFO Utils: Fetching spark://192.168.7.5:40651/jars/aws-java-sdk-bundle-1.12.508.jar to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/fetchFileTemp16013748398762089416.tmp
[2024-12-12T11:08:00.742-0300] {subprocess.py:93} INFO - 24/12/12 11:08:00 INFO Executor: Adding file:/tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/aws-java-sdk-bundle-1.12.508.jar to class loader default
[2024-12-12T11:08:00.754-0300] {subprocess.py:93} INFO - 24/12/12 11:08:00 INFO Executor: Fetching spark://192.168.7.5:40651/jars/hadoop-aws-3.3.4.jar with timestamp 1734012462717
[2024-12-12T11:08:00.760-0300] {subprocess.py:93} INFO - 24/12/12 11:08:00 INFO Utils: Fetching spark://192.168.7.5:40651/jars/hadoop-aws-3.3.4.jar to /tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/fetchFileTemp6554564218006131118.tmp
[2024-12-12T11:08:00.963-0300] {subprocess.py:93} INFO - 24/12/12 11:08:00 INFO Executor: Adding file:/tmp/spark-78fbe6c7-08e0-42dd-8031-49c436060754/userFiles-b0f9dc99-219e-48b7-836b-9e6034563e35/hadoop-aws-3.3.4.jar to class loader default
[2024-12-12T11:08:01.003-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42939.
[2024-12-12T11:08:01.005-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO NettyBlockTransferService: Server created on 192.168.7.5:42939
[2024-12-12T11:08:01.014-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-12-12T11:08:01.094-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.7.5, 42939, None)
[2024-12-12T11:08:01.106-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.7.5:42939 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.7.5, 42939, None)
[2024-12-12T11:08:01.118-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.7.5, 42939, None)
[2024-12-12T11:08:01.133-0300] {subprocess.py:93} INFO - 24/12/12 11:08:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.7.5, 42939, None)
[2024-12-12T11:08:03.230-0300] {subprocess.py:93} INFO - 24/12/12 11:08:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-12-12T11:08:03.264-0300] {subprocess.py:93} INFO - 24/12/12 11:08:03 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmprdpo_e8i/spark-warehouse'.
[2024-12-12T11:08:10.484-0300] {subprocess.py:93} INFO - 24/12/12 11:08:10 INFO InMemoryFileIndex: It took 397 ms to list leaf files for 1 paths.
[2024-12-12T11:08:11.090-0300] {subprocess.py:93} INFO - 24/12/12 11:08:11 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
[2024-12-12T11:09:01.561-0300] {subprocess.py:93} INFO - 24/12/12 11:09:01 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:09:01.579-0300] {subprocess.py:93} INFO - 24/12/12 11:09:01 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:09:01.945-0300] {subprocess.py:93} INFO - 24/12/12 11:09:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 217.4 KiB, free 434.2 MiB)
[2024-12-12T11:09:02.017-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.2 MiB)
[2024-12-12T11:09:02.020-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.7.5:42939 (size: 37.1 KiB, free: 434.4 MiB)
[2024-12-12T11:09:02.025-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:02.039-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:09:02.267-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:02.286-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:09:02.286-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:09:02.287-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:09:02.289-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:09:02.294-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:09:02.420-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.8 KiB, free 434.1 MiB)
[2024-12-12T11:09:02.427-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.1 MiB)
[2024-12-12T11:09:02.428-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.7.5:42939 (size: 8.2 KiB, free: 434.4 MiB)
[2024-12-12T11:09:02.428-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:09:02.444-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:09:02.445-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-12-12T11:09:02.499-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10325 bytes)
[2024-12-12T11:09:02.512-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-12-12T11:09:02.603-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/customers.json, range: 0-3105168, partition values: [empty row]
[2024-12-12T11:09:02.935-0300] {subprocess.py:93} INFO - 24/12/12 11:09:02 INFO CodeGenerator: Code generated in 293.380855 ms
[2024-12-12T11:09:03.180-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2330 bytes result sent to driver
[2024-12-12T11:09:03.194-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 706 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:09:03.196-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-12-12T11:09:03.202-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,862 s
[2024-12-12T11:09:03.206-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:09:03.206-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-12-12T11:09:03.208-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,941191 s
[2024-12-12T11:09:03.474-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[2024-12-12T11:09:03.477-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.7.5:42939 in memory (size: 8.2 KiB, free: 434.4 MiB)
[2024-12-12T11:09:03.482-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.7.5:42939 in memory (size: 37.1 KiB, free: 434.4 MiB)
[2024-12-12T11:09:03.502-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2024-12-12T11:09:03.503-0300] {subprocess.py:93} INFO - 24/12/12 11:09:03 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2024-12-12T11:09:04.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:04 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:09:04.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:04 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:09:05.749-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:05.834-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:05.834-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:05.835-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:05.836-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:05.836-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:05.837-0300] {subprocess.py:93} INFO - 24/12/12 11:09:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:07.794-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 217.3 KiB, free 434.2 MiB)
[2024-12-12T11:09:07.821-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 434.2 MiB)
[2024-12-12T11:09:07.822-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.7.5:42939 (size: 37.0 KiB, free: 434.4 MiB)
[2024-12-12T11:09:07.824-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO SparkContext: Created broadcast 2 from parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:07.847-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:09:07.884-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:07.886-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:09:07.886-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:09:07.887-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:09:07.888-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:09:07.890-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:09:07.922-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.7 KiB, free 433.9 MiB)
[2024-12-12T11:09:07.927-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 81.9 KiB, free 433.9 MiB)
[2024-12-12T11:09:07.928-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.7.5:42939 (size: 81.9 KiB, free: 434.3 MiB)
[2024-12-12T11:09:07.928-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:09:07.930-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:09:07.930-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-12-12T11:09:07.944-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10554 bytes)
[2024-12-12T11:09:07.945-0300] {subprocess.py:93} INFO - 24/12/12 11:09:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-12-12T11:09:08.029-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:08.029-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:08.030-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:08.030-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:08.030-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:08.030-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:08.035-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:09:08.038-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:09:08.064-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-12-12T11:09:08.122-0300] {subprocess.py:93} INFO - 24/12/12 11:09:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-12-12T11:09:08.122-0300] {subprocess.py:93} INFO - {
[2024-12-12T11:09:08.122-0300] {subprocess.py:93} INFO -   "type" : "struct",
[2024-12-12T11:09:08.122-0300] {subprocess.py:93} INFO -   "fields" : [ {
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "name" : "customer_city",
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "name" : "customer_email",
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.123-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "name" : "customer_fname",
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "name" : "customer_id",
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.124-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "name" : "customer_lname",
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "name" : "customer_password",
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.125-0300] {subprocess.py:93} INFO -     "name" : "customer_state",
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "name" : "customer_street",
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.126-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO -     "name" : "customer_zipcode",
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO -   } ]
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO - message spark_schema {
[2024-12-12T11:09:08.127-0300] {subprocess.py:93} INFO -   optional binary customer_city (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_email (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_fname (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional int64 customer_id;
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_lname (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_password (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_state (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_street (STRING);
[2024-12-12T11:09:08.128-0300] {subprocess.py:93} INFO -   optional binary customer_zipcode (STRING);
[2024-12-12T11:09:08.129-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:09:08.129-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:09:08.129-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:09:09.162-0300] {subprocess.py:93} INFO - 24/12/12 11:09:09 INFO CodecPool: Got brand-new compressor [.snappy]
[2024-12-12T11:09:09.348-0300] {subprocess.py:93} INFO - 24/12/12 11:09:09 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/customers.json, range: 0-3105168, partition values: [empty row]
[2024-12-12T11:09:09.398-0300] {subprocess.py:93} INFO - 24/12/12 11:09:09 INFO CodeGenerator: Code generated in 26.457459 ms
[2024-12-12T11:09:18.035-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO FileOutputCommitter: Saved output of task 'attempt_202412121109076313335234154619527_0001_m_000000_1' to s3a://bw-airflow/lakehouse/bronze/customers.parquet/_temporary/0/task_202412121109076313335234154619527_0001_m_000000
[2024-12-12T11:09:18.055-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO SparkHadoopMapRedUtil: attempt_202412121109076313335234154619527_0001_m_000000_1: Committed. Elapsed time: 4839 ms.
[2024-12-12T11:09:18.065-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver
[2024-12-12T11:09:18.068-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10137 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:09:18.069-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-12-12T11:09:18.070-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 10,179 s
[2024-12-12T11:09:18.071-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:09:18.071-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-12-12T11:09:18.072-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 10,187303 s
[2024-12-12T11:09:18.074-0300] {subprocess.py:93} INFO - 24/12/12 11:09:18 INFO FileFormatWriter: Start to commit write Job dd1f0749-b645-424b-82f3-ce6193799d4a.
[2024-12-12T11:09:26.505-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileFormatWriter: Write Job dd1f0749-b645-424b-82f3-ce6193799d4a committed. Elapsed time: 8417 ms.
[2024-12-12T11:09:26.517-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileFormatWriter: Finished processing stats for write job dd1f0749-b645-424b-82f3-ce6193799d4a.
[2024-12-12T11:09:26.528-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-12-12T11:09:26.534-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-12-12T11:09:26.574-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:09:26.574-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:09:26.579-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 217.4 KiB, free 433.6 MiB)
[2024-12-12T11:09:26.594-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 433.6 MiB)
[2024-12-12T11:09:26.595-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.7.5:42939 (size: 37.1 KiB, free: 434.2 MiB)
[2024-12-12T11:09:26.596-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:26.597-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:09:26.612-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:26.613-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 7 output partitions
[2024-12-12T11:09:26.613-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:09:26.613-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:09:26.614-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:09:26.615-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:09:26.619-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 17.8 KiB, free 433.6 MiB)
[2024-12-12T11:09:26.620-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 433.6 MiB)
[2024-12-12T11:09:26.621-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.7.5:42939 (size: 8.2 KiB, free: 434.2 MiB)
[2024-12-12T11:09:26.621-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:09:26.622-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
[2024-12-12T11:09:26.622-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 7 tasks resource profile 0
[2024-12-12T11:09:26.624-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.624-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (192.168.7.5, executor driver, partition 1, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.625-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (192.168.7.5, executor driver, partition 2, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.626-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (192.168.7.5, executor driver, partition 3, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.626-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6) (192.168.7.5, executor driver, partition 4, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.627-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7) (192.168.7.5, executor driver, partition 5, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.628-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8) (192.168.7.5, executor driver, partition 6, PROCESS_LOCAL, 10326 bytes)
[2024-12-12T11:09:26.629-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-12-12T11:09:26.630-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
[2024-12-12T11:09:26.631-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 3.0 in stage 2.0 (TID 5)
[2024-12-12T11:09:26.632-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)
[2024-12-12T11:09:26.633-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 6.0 in stage 2.0 (TID 8)
[2024-12-12T11:09:26.634-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 4.0 in stage 2.0 (TID 6)
[2024-12-12T11:09:26.634-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO Executor: Running task 5.0 in stage 2.0 (TID 7)
[2024-12-12T11:09:26.654-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 0-4194304, partition values: [empty row]
[2024-12-12T11:09:26.659-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 25165824-28655610, partition values: [empty row]
[2024-12-12T11:09:26.661-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.7.5:42939 in memory (size: 81.9 KiB, free: 434.3 MiB)
[2024-12-12T11:09:26.663-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 20971520-25165824, partition values: [empty row]
[2024-12-12T11:09:26.664-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 12582912-16777216, partition values: [empty row]
[2024-12-12T11:09:26.668-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 4194304-8388608, partition values: [empty row]
[2024-12-12T11:09:26.674-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 8388608-12582912, partition values: [empty row]
[2024-12-12T11:09:26.675-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 16777216-20971520, partition values: [empty row]
[2024-12-12T11:09:26.932-0300] {subprocess.py:93} INFO - 24/12/12 11:09:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.7.5:42939 in memory (size: 37.0 KiB, free: 434.4 MiB)
[2024-12-12T11:09:27.000-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 6.0 in stage 2.0 (TID 8). 2255 bytes result sent to driver
[2024-12-12T11:09:27.011-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 8) in 383 ms on 192.168.7.5 (executor driver) (1/7)
[2024-12-12T11:09:27.126-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 2255 bytes result sent to driver
[2024-12-12T11:09:27.129-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 505 ms on 192.168.7.5 (executor driver) (2/7)
[2024-12-12T11:09:27.141-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 2212 bytes result sent to driver
[2024-12-12T11:09:27.144-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 519 ms on 192.168.7.5 (executor driver) (3/7)
[2024-12-12T11:09:27.144-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2212 bytes result sent to driver
[2024-12-12T11:09:27.145-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 5.0 in stage 2.0 (TID 7). 2212 bytes result sent to driver
[2024-12-12T11:09:27.151-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 526 ms on 192.168.7.5 (executor driver) (4/7)
[2024-12-12T11:09:27.151-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 524 ms on 192.168.7.5 (executor driver) (5/7)
[2024-12-12T11:09:27.159-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 4.0 in stage 2.0 (TID 6). 2212 bytes result sent to driver
[2024-12-12T11:09:27.160-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 6) in 534 ms on 192.168.7.5 (executor driver) (6/7)
[2024-12-12T11:09:27.175-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO Executor: Finished task 3.0 in stage 2.0 (TID 5). 2212 bytes result sent to driver
[2024-12-12T11:09:27.177-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 551 ms on 192.168.7.5 (executor driver) (7/7)
[2024-12-12T11:09:27.189-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-12-12T11:09:27.190-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,573 s
[2024-12-12T11:09:27.190-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:09:27.190-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-12-12T11:09:27.191-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,578879 s
[2024-12-12T11:09:27.235-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:09:27.236-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:09:27.724-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:27.726-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:27.726-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:27.727-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:27.727-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:27.728-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:27.728-0300] {subprocess.py:93} INFO - 24/12/12 11:09:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:29.490-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 217.3 KiB, free 433.9 MiB)
[2024-12-12T11:09:29.512-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.7.5:42939 in memory (size: 8.2 KiB, free: 434.4 MiB)
[2024-12-12T11:09:29.512-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.7.5:42939 in memory (size: 37.1 KiB, free: 434.4 MiB)
[2024-12-12T11:09:29.513-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 434.2 MiB)
[2024-12-12T11:09:29.513-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.7.5:42939 (size: 37.0 KiB, free: 434.4 MiB)
[2024-12-12T11:09:29.513-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO SparkContext: Created broadcast 6 from parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:29.513-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:09:29.521-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:29.522-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:09:29.523-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:09:29.523-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:09:29.524-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:09:29.525-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:09:29.553-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 221.1 KiB, free 433.9 MiB)
[2024-12-12T11:09:29.555-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 81.7 KiB, free 433.9 MiB)
[2024-12-12T11:09:29.556-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.7.5:42939 (size: 81.7 KiB, free: 434.3 MiB)
[2024-12-12T11:09:29.557-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:09:29.558-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:09:29.558-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-12-12T11:09:29.560-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 11053 bytes)
[2024-12-12T11:09:29.561-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)
[2024-12-12T11:09:29.582-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:29.583-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:29.583-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:29.583-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:29.583-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:29.584-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:29.584-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:09:29.584-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:09:29.587-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-12-12T11:09:29.589-0300] {subprocess.py:93} INFO - 24/12/12 11:09:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-12-12T11:09:29.590-0300] {subprocess.py:93} INFO - {
[2024-12-12T11:09:29.590-0300] {subprocess.py:93} INFO -   "type" : "struct",
[2024-12-12T11:09:29.590-0300] {subprocess.py:93} INFO -   "fields" : [ {
[2024-12-12T11:09:29.590-0300] {subprocess.py:93} INFO -     "name" : "order_item_id",
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -     "name" : "order_item_order_id",
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:29.591-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -     "name" : "order_item_product_id",
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:29.592-0300] {subprocess.py:93} INFO -     "name" : "order_item_product_price",
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "type" : "double",
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "name" : "order_item_quantity",
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:29.593-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO -     "name" : "order_item_subtotal",
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO -     "type" : "double",
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO -   } ]
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2024-12-12T11:09:29.594-0300] {subprocess.py:93} INFO - message spark_schema {
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO -   optional int64 order_item_id;
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO -   optional int64 order_item_order_id;
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO -   optional int64 order_item_product_id;
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO -   optional double order_item_product_price;
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO -   optional int64 order_item_quantity;
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO -   optional double order_item_subtotal;
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:09:29.595-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:09:29.596-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:09:30.542-0300] {subprocess.py:93} INFO - 24/12/12 11:09:30 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 0-4194304, partition values: [empty row]
[2024-12-12T11:09:30.641-0300] {subprocess.py:93} INFO - 24/12/12 11:09:30 INFO CodeGenerator: Code generated in 23.839448 ms
[2024-12-12T11:09:30.989-0300] {subprocess.py:93} INFO - 24/12/12 11:09:30 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 4194304-8388608, partition values: [empty row]
[2024-12-12T11:09:31.193-0300] {subprocess.py:93} INFO - 24/12/12 11:09:31 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 8388608-12582912, partition values: [empty row]
[2024-12-12T11:09:31.380-0300] {subprocess.py:93} INFO - 24/12/12 11:09:31 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 12582912-16777216, partition values: [empty row]
[2024-12-12T11:09:31.603-0300] {subprocess.py:93} INFO - 24/12/12 11:09:31 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 16777216-20971520, partition values: [empty row]
[2024-12-12T11:09:31.747-0300] {subprocess.py:93} INFO - 24/12/12 11:09:31 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 20971520-25165824, partition values: [empty row]
[2024-12-12T11:09:31.863-0300] {subprocess.py:93} INFO - 24/12/12 11:09:31 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/order_item.json, range: 25165824-28655610, partition values: [empty row]
[2024-12-12T11:09:40.159-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO FileOutputCommitter: Saved output of task 'attempt_202412121109295046680423424921603_0003_m_000000_9' to s3a://bw-airflow/lakehouse/bronze/order_item.parquet/_temporary/0/task_202412121109295046680423424921603_0003_m_000000
[2024-12-12T11:09:40.214-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO SparkHadoopMapRedUtil: attempt_202412121109295046680423424921603_0003_m_000000_9: Committed. Elapsed time: 4745 ms.
[2024-12-12T11:09:40.214-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 2545 bytes result sent to driver
[2024-12-12T11:09:40.214-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 10611 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:09:40.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-12-12T11:09:40.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 10,645 s
[2024-12-12T11:09:40.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:09:40.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-12-12T11:09:40.215-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 10,651052 s
[2024-12-12T11:09:40.216-0300] {subprocess.py:93} INFO - 24/12/12 11:09:40 INFO FileFormatWriter: Start to commit write Job 53e81eb0-33ed-4d13-9cc6-5d3c4213c3e0.
[2024-12-12T11:09:48.232-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileFormatWriter: Write Job 53e81eb0-33ed-4d13-9cc6-5d3c4213c3e0 committed. Elapsed time: 8032 ms.
[2024-12-12T11:09:48.309-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileFormatWriter: Finished processing stats for write job 53e81eb0-33ed-4d13-9cc6-5d3c4213c3e0.
[2024-12-12T11:09:48.328-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2024-12-12T11:09:48.340-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-12-12T11:09:48.401-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:09:48.401-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:09:48.405-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 217.4 KiB, free 433.6 MiB)
[2024-12-12T11:09:48.429-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 433.6 MiB)
[2024-12-12T11:09:48.430-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.7.5:42939 (size: 37.1 KiB, free: 434.2 MiB)
[2024-12-12T11:09:48.431-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO SparkContext: Created broadcast 8 from json at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:48.432-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:09:48.448-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:48.450-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Got job 4 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-12-12T11:09:48.451-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Final stage: ResultStage 4 (json at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:09:48.451-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:09:48.463-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:09:48.466-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:09:48.471-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.8 KiB, free 433.6 MiB)
[2024-12-12T11:09:48.473-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 433.6 MiB)
[2024-12-12T11:09:48.473-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.7.5:42939 (size: 8.2 KiB, free: 434.2 MiB)
[2024-12-12T11:09:48.474-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:09:48.475-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-12-12T11:09:48.475-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2024-12-12T11:09:48.477-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10322 bytes)
[2024-12-12T11:09:48.477-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 11) (192.168.7.5, executor driver, partition 1, PROCESS_LOCAL, 10322 bytes)
[2024-12-12T11:09:48.478-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO Executor: Running task 1.0 in stage 4.0 (TID 11)
[2024-12-12T11:09:48.478-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO Executor: Running task 0.0 in stage 4.0 (TID 10)
[2024-12-12T11:09:48.500-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/orders.json, range: 4194304-7477339, partition values: [empty row]
[2024-12-12T11:09:48.507-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/orders.json, range: 0-4194304, partition values: [empty row]
[2024-12-12T11:09:48.508-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.7.5:42939 in memory (size: 81.7 KiB, free: 434.3 MiB)
[2024-12-12T11:09:48.790-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO Executor: Finished task 1.0 in stage 4.0 (TID 11). 2104 bytes result sent to driver
[2024-12-12T11:09:48.799-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO Executor: Finished task 0.0 in stage 4.0 (TID 10). 2147 bytes result sent to driver
[2024-12-12T11:09:48.799-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 281 ms on 192.168.7.5 (executor driver) (1/2)
[2024-12-12T11:09:48.800-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 11) in 280 ms on 192.168.7.5 (executor driver) (2/2)
[2024-12-12T11:09:48.800-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-12-12T11:09:48.800-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: ResultStage 4 (json at NativeMethodAccessorImpl.java:0) finished in 0,290 s
[2024-12-12T11:09:48.800-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:09:48.800-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-12-12T11:09:48.800-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO DAGScheduler: Job 4 finished: json at NativeMethodAccessorImpl.java:0, took 0,310965 s
[2024-12-12T11:09:48.819-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.7.5:42939 in memory (size: 37.0 KiB, free: 434.4 MiB)
[2024-12-12T11:09:48.835-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:09:48.836-0300] {subprocess.py:93} INFO - 24/12/12 11:09:48 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:09:49.237-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:49.250-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:49.250-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:49.251-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:49.251-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:49.251-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:49.252-0300] {subprocess.py:93} INFO - 24/12/12 11:09:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:51.094-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 217.3 KiB, free 433.9 MiB)
[2024-12-12T11:09:51.115-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.7.5:42939 in memory (size: 37.1 KiB, free: 434.4 MiB)
[2024-12-12T11:09:51.118-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.7.5:42939 in memory (size: 8.2 KiB, free: 434.4 MiB)
[2024-12-12T11:09:51.119-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 434.2 MiB)
[2024-12-12T11:09:51.119-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.7.5:42939 (size: 37.0 KiB, free: 434.4 MiB)
[2024-12-12T11:09:51.120-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO SparkContext: Created broadcast 10 from parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:51.121-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:09:51.129-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:09:51.130-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:09:51.131-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:09:51.131-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:09:51.132-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:09:51.133-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:09:51.169-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 220.6 KiB, free 433.9 MiB)
[2024-12-12T11:09:51.172-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 81.4 KiB, free 433.9 MiB)
[2024-12-12T11:09:51.172-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.7.5:42939 (size: 81.4 KiB, free: 434.3 MiB)
[2024-12-12T11:09:51.173-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:09:51.174-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:09:51.174-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-12-12T11:09:51.175-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 12) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10634 bytes)
[2024-12-12T11:09:51.176-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 12)
[2024-12-12T11:09:51.191-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:51.192-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:51.192-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:51.192-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:09:51.193-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:09:51.193-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:09:51.193-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:09:51.194-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:09:51.196-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-12-12T11:09:51.197-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-12-12T11:09:51.197-0300] {subprocess.py:93} INFO - {
[2024-12-12T11:09:51.198-0300] {subprocess.py:93} INFO -   "type" : "struct",
[2024-12-12T11:09:51.198-0300] {subprocess.py:93} INFO -   "fields" : [ {
[2024-12-12T11:09:51.198-0300] {subprocess.py:93} INFO -     "name" : "order_customer_id",
[2024-12-12T11:09:51.198-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:51.199-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:51.199-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:51.199-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:51.199-0300] {subprocess.py:93} INFO -     "name" : "order_date",
[2024-12-12T11:09:51.199-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:51.199-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -     "name" : "order_id",
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:09:51.200-0300] {subprocess.py:93} INFO -     "name" : "order_status",
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO -   } ]
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO - message spark_schema {
[2024-12-12T11:09:51.201-0300] {subprocess.py:93} INFO -   optional int64 order_customer_id;
[2024-12-12T11:09:51.202-0300] {subprocess.py:93} INFO -   optional binary order_date (STRING);
[2024-12-12T11:09:51.202-0300] {subprocess.py:93} INFO -   optional int64 order_id;
[2024-12-12T11:09:51.202-0300] {subprocess.py:93} INFO -   optional binary order_status (STRING);
[2024-12-12T11:09:51.202-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:09:51.202-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:09:51.202-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:09:52.013-0300] {subprocess.py:93} INFO - 24/12/12 11:09:51 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/orders.json, range: 0-4194304, partition values: [empty row]
[2024-12-12T11:09:52.026-0300] {subprocess.py:93} INFO - 24/12/12 11:09:52 INFO CodeGenerator: Code generated in 11.944012 ms
[2024-12-12T11:09:52.297-0300] {subprocess.py:93} INFO - 24/12/12 11:09:52 INFO FileScanRDD: Reading File path: file:///home/beatriz/Documentos/airflow/lakehouse/landing/orders.json, range: 4194304-7477339, partition values: [empty row]
[2024-12-12T11:10:01.317-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO FileOutputCommitter: Saved output of task 'attempt_202412121109512499684815919732335_0005_m_000000_12' to s3a://bw-airflow/lakehouse/bronze/orders.parquet/_temporary/0/task_202412121109512499684815919732335_0005_m_000000
[2024-12-12T11:10:01.340-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO SparkHadoopMapRedUtil: attempt_202412121109512499684815919732335_0005_m_000000_12: Committed. Elapsed time: 5419 ms.
[2024-12-12T11:10:01.340-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 12). 2545 bytes result sent to driver
[2024-12-12T11:10:01.340-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 12) in 10148 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:10:01.341-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-12-12T11:10:01.341-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 10,189 s
[2024-12-12T11:10:01.341-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:10:01.341-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-12-12T11:10:01.341-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 10,195614 s
[2024-12-12T11:10:01.341-0300] {subprocess.py:93} INFO - 24/12/12 11:10:01 INFO FileFormatWriter: Start to commit write Job e5fd3b33-76f7-4683-ae78-f2e77a408b05.
[2024-12-12T11:10:09.520-0300] {subprocess.py:93} INFO - 24/12/12 11:10:09 INFO FileFormatWriter: Write Job e5fd3b33-76f7-4683-ae78-f2e77a408b05 committed. Elapsed time: 8177 ms.
[2024-12-12T11:10:09.530-0300] {subprocess.py:93} INFO - 24/12/12 11:10:09 INFO FileFormatWriter: Finished processing stats for write job e5fd3b33-76f7-4683-ae78-f2e77a408b05.
[2024-12-12T11:10:10.757-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO InMemoryFileIndex: It took 207 ms to list leaf files for 1 paths.
[2024-12-12T11:10:10.910-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:10:10.911-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:10:10.911-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:10:10.911-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:10:10.912-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:10:10.912-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:10:10.912-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 107.3 KiB, free 433.8 MiB)
[2024-12-12T11:10:10.935-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)
[2024-12-12T11:10:10.937-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.7.5:42939 (size: 39.9 KiB, free: 434.2 MiB)
[2024-12-12T11:10:10.941-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.7.5:42939 in memory (size: 81.4 KiB, free: 434.3 MiB)
[2024-12-12T11:10:10.941-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:10:10.943-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:10:10.944-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-12-12T11:10:10.955-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 13) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 9867 bytes)
[2024-12-12T11:10:10.956-0300] {subprocess.py:93} INFO - 24/12/12 11:10:10 INFO Executor: Running task 0.0 in stage 6.0 (TID 13)
[2024-12-12T11:10:11.518-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:10:11.829-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 13). 2127 bytes result sent to driver
[2024-12-12T11:10:11.848-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 13) in 889 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:10:11.849-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-12-12T11:10:11.849-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 0,935 s
[2024-12-12T11:10:11.849-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:10:11.850-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-12-12T11:10:11.851-0300] {subprocess.py:93} INFO - 24/12/12 11:10:11 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 0,940510 s
[2024-12-12T11:10:12.043-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:10:12.051-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:10:12.580-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:10:12.612-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:10:12.612-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:10:12.612-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:10:12.612-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:10:12.613-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:10:12.613-0300] {subprocess.py:93} INFO - 24/12/12 11:10:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:10:14.622-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO CodeGenerator: Code generated in 67.592914 ms
[2024-12-12T11:10:14.637-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 220.2 KiB, free 433.8 MiB)
[2024-12-12T11:10:14.643-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.7.5:42939 in memory (size: 39.9 KiB, free: 434.4 MiB)
[2024-12-12T11:10:14.651-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 37.9 KiB, free 433.9 MiB)
[2024-12-12T11:10:14.652-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.7.5:42939 (size: 37.9 KiB, free: 434.3 MiB)
[2024-12-12T11:10:14.653-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO SparkContext: Created broadcast 13 from parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:10:14.659-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:10:14.692-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:10:14.696-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO DAGScheduler: Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:10:14.696-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:10:14.697-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:10:14.701-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:10:14.708-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:10:14.795-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 229.2 KiB, free 433.7 MiB)
[2024-12-12T11:10:14.807-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 83.2 KiB, free 433.6 MiB)
[2024-12-12T11:10:14.808-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.7.5:42939 (size: 83.2 KiB, free: 434.2 MiB)
[2024-12-12T11:10:14.810-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:10:14.812-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:10:14.813-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-12-12T11:10:14.816-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10601 bytes)
[2024-12-12T11:10:14.820-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO Executor: Running task 0.0 in stage 7.0 (TID 14)
[2024-12-12T11:10:14.882-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:10:14.882-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:10:14.883-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:10:14.883-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:10:14.884-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:10:14.884-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:10:14.884-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:10:14.885-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:10:14.886-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-12-12T11:10:14.889-0300] {subprocess.py:93} INFO - 24/12/12 11:10:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-12-12T11:10:14.890-0300] {subprocess.py:93} INFO - {
[2024-12-12T11:10:14.891-0300] {subprocess.py:93} INFO -   "type" : "struct",
[2024-12-12T11:10:14.891-0300] {subprocess.py:93} INFO -   "fields" : [ {
[2024-12-12T11:10:14.891-0300] {subprocess.py:93} INFO -     "name" : "city",
[2024-12-12T11:10:14.892-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.892-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.892-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.893-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.893-0300] {subprocess.py:93} INFO -     "name" : "email",
[2024-12-12T11:10:14.893-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.894-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.894-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.894-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.895-0300] {subprocess.py:93} INFO -     "name" : "fname",
[2024-12-12T11:10:14.895-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.912-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.912-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.913-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.913-0300] {subprocess.py:93} INFO -     "name" : "id",
[2024-12-12T11:10:14.913-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:10:14.913-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.914-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.914-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.914-0300] {subprocess.py:93} INFO -     "name" : "lname",
[2024-12-12T11:10:14.914-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.914-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.915-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.915-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.915-0300] {subprocess.py:93} INFO -     "name" : "password",
[2024-12-12T11:10:14.915-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.916-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.916-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.916-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.917-0300] {subprocess.py:93} INFO -     "name" : "state",
[2024-12-12T11:10:14.917-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.917-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.918-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.918-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.918-0300] {subprocess.py:93} INFO -     "name" : "street",
[2024-12-12T11:10:14.918-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.919-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.919-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.919-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:10:14.920-0300] {subprocess.py:93} INFO -     "name" : "zipcode",
[2024-12-12T11:10:14.920-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:10:14.920-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:10:14.921-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:10:14.921-0300] {subprocess.py:93} INFO -   } ]
[2024-12-12T11:10:14.921-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:10:14.921-0300] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2024-12-12T11:10:14.922-0300] {subprocess.py:93} INFO - message spark_schema {
[2024-12-12T11:10:14.922-0300] {subprocess.py:93} INFO -   optional binary city (STRING);
[2024-12-12T11:10:14.922-0300] {subprocess.py:93} INFO -   optional binary email (STRING);
[2024-12-12T11:10:14.923-0300] {subprocess.py:93} INFO -   optional binary fname (STRING);
[2024-12-12T11:10:14.923-0300] {subprocess.py:93} INFO -   optional int64 id;
[2024-12-12T11:10:14.924-0300] {subprocess.py:93} INFO -   optional binary lname (STRING);
[2024-12-12T11:10:14.924-0300] {subprocess.py:93} INFO -   optional binary password (STRING);
[2024-12-12T11:10:14.924-0300] {subprocess.py:93} INFO -   optional binary state (STRING);
[2024-12-12T11:10:14.925-0300] {subprocess.py:93} INFO -   optional binary street (STRING);
[2024-12-12T11:10:14.925-0300] {subprocess.py:93} INFO -   optional binary zipcode (STRING);
[2024-12-12T11:10:14.925-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:10:14.926-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:10:14.926-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:10:15.935-0300] {subprocess.py:93} INFO - 24/12/12 11:10:15 INFO CodeGenerator: Code generated in 49.974414 ms
[2024-12-12T11:10:15.966-0300] {subprocess.py:93} INFO - 24/12/12 11:10:15 INFO FileScanRDD: Reading File path: s3a://bw-airflow/lakehouse/bronze/customers.parquet/part-00000-20e07c82-a015-4a45-b872-ede97f95a13f-c000.snappy.parquet, range: 0-252779, partition values: [empty row]
[2024-12-12T11:10:16.573-0300] {subprocess.py:93} INFO - 24/12/12 11:10:16 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:10:18.468-0300] {subprocess.py:93} INFO - 24/12/12 11:10:18 INFO CodecPool: Got brand-new decompressor [.snappy]
[2024-12-12T11:12:12.593-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO FileOutputCommitter: Saved output of task 'attempt_202412121110142992446658143364017_0007_m_000000_14' to s3a://bw-airflow/lakehouse/silver/customers.parquet/_temporary/0/task_202412121110142992446658143364017_0007_m_000000
[2024-12-12T11:12:12.646-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO SparkHadoopMapRedUtil: attempt_202412121110142992446658143364017_0007_m_000000_14: Committed. Elapsed time: 111063 ms.
[2024-12-12T11:12:12.677-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO Executor: Finished task 0.0 in stage 7.0 (TID 14). 2821 bytes result sent to driver
[2024-12-12T11:12:12.726-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 117852 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:12:12.727-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-12-12T11:12:12.727-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 117,963 s
[2024-12-12T11:12:12.728-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:12:12.728-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-12-12T11:12:12.728-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO DAGScheduler: Job 7 finished: parquet at NativeMethodAccessorImpl.java:0, took 117,979760 s
[2024-12-12T11:12:12.728-0300] {subprocess.py:93} INFO - 24/12/12 11:12:12 INFO FileFormatWriter: Start to commit write Job 0920cddf-ad13-4592-a6fc-069cf717bf04.
[2024-12-12T11:12:24.540-0300] {subprocess.py:93} INFO - 24/12/12 11:12:24 INFO FileFormatWriter: Write Job 0920cddf-ad13-4592-a6fc-069cf717bf04 committed. Elapsed time: 11821 ms.
[2024-12-12T11:12:25.574-0300] {subprocess.py:93} INFO - 24/12/12 11:12:24 INFO FileFormatWriter: Finished processing stats for write job 0920cddf-ad13-4592-a6fc-069cf717bf04.
[2024-12-12T11:12:33.465-0300] {subprocess.py:93} INFO - 24/12/12 11:12:33 INFO InMemoryFileIndex: It took 1310 ms to list leaf files for 1 paths.
[2024-12-12T11:12:39.325-0300] {subprocess.py:93} INFO - 24/12/12 11:12:38 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:12:47.333-0300] {subprocess.py:93} INFO - 24/12/12 11:12:38 INFO DAGScheduler: Got job 8 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:13:04.131-0300] {subprocess.py:93} INFO - 24/12/12 11:12:38 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:13:15.462-0300] {subprocess.py:93} INFO - 24/12/12 11:12:38 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:13:16.708-0300] {subprocess.py:93} INFO - 24/12/12 11:12:38 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:13:17.978-0300] {subprocess.py:93} INFO - 24/12/12 11:12:39 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[32] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:13:18.545-0300] {subprocess.py:93} INFO - 24/12/12 11:12:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 107.3 KiB, free 433.5 MiB)
[2024-12-12T11:13:19.251-0300] {subprocess.py:93} INFO - 24/12/12 11:12:45 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)
[2024-12-12T11:13:19.453-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.7.5:42939 (size: 39.9 KiB, free: 434.2 MiB)
[2024-12-12T11:13:19.675-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:13:19.675-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[32] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:13:19.676-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-12-12T11:13:19.676-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.7.5:42939 in memory (size: 37.9 KiB, free: 434.2 MiB)
[2024-12-12T11:13:19.677-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 9868 bytes)
[2024-12-12T11:13:19.677-0300] {subprocess.py:93} INFO - 24/12/12 11:12:46 INFO Executor: Running task 0.0 in stage 8.0 (TID 15)
[2024-12-12T11:13:19.677-0300] {subprocess.py:93} INFO - 24/12/12 11:12:48 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.7.5:42939 in memory (size: 83.2 KiB, free: 434.3 MiB)
[2024-12-12T11:13:29.026-0300] {subprocess.py:93} INFO - 24/12/12 11:13:29 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:13:30.519-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 15). 2021 bytes result sent to driver
[2024-12-12T11:13:30.544-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 43731 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:13:30.545-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-12-12T11:13:30.545-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) finished in 50,882 s
[2024-12-12T11:13:30.546-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:13:30.546-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-12-12T11:13:30.546-0300] {subprocess.py:93} INFO - 24/12/12 11:13:30 INFO DAGScheduler: Job 8 finished: parquet at NativeMethodAccessorImpl.java:0, took 51,712573 s
[2024-12-12T11:13:31.112-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:13:31.224-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:13:31.608-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:13:31.645-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:13:31.646-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:13:31.646-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:13:31.646-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:13:31.647-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:13:31.647-0300] {subprocess.py:93} INFO - 24/12/12 11:13:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:13:34.242-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO CodeGenerator: Code generated in 46.057719 ms
[2024-12-12T11:13:34.362-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 219.8 KiB, free 433.8 MiB)
[2024-12-12T11:13:34.870-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.7.5:42939 in memory (size: 39.9 KiB, free: 434.4 MiB)
[2024-12-12T11:13:34.883-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 37.8 KiB, free 433.9 MiB)
[2024-12-12T11:13:34.884-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.7.5:42939 (size: 37.8 KiB, free: 434.3 MiB)
[2024-12-12T11:13:34.885-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO SparkContext: Created broadcast 16 from parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:13:34.886-0300] {subprocess.py:93} INFO - 24/12/12 11:13:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:13:35.024-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:13:35.127-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:13:35.141-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:13:35.141-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:13:35.142-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:13:35.142-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:13:35.256-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 227.0 KiB, free 433.7 MiB)
[2024-12-12T11:13:35.659-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 82.9 KiB, free 433.6 MiB)
[2024-12-12T11:13:35.693-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.7.5:42939 (size: 82.9 KiB, free: 434.2 MiB)
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 16) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10602 bytes)
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO Executor: Running task 0.0 in stage 9.0 (TID 16)
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:13:35.736-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-12-12T11:13:35.737-0300] {subprocess.py:93} INFO - 24/12/12 11:13:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO - {
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO -   "type" : "struct",
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO -   "fields" : [ {
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO -     "name" : "id",
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:13:35.738-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "name" : "order_id",
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "name" : "product_id",
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:13:35.739-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "name" : "product_price",
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "type" : "double",
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "name" : "quantity",
[2024-12-12T11:13:35.740-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -     "name" : "subtotal",
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -     "type" : "double",
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO -   } ]
[2024-12-12T11:13:35.741-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO - message spark_schema {
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO -   optional int64 id;
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO -   optional int64 order_id;
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO -   optional int64 product_id;
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO -   optional double product_price;
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO -   optional int64 quantity;
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO -   optional double subtotal;
[2024-12-12T11:13:35.742-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:13:35.743-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:13:35.743-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:13:41.411-0300] {subprocess.py:93} INFO - 24/12/12 11:13:41 INFO CodeGenerator: Code generated in 92.411632 ms
[2024-12-12T11:13:41.896-0300] {subprocess.py:93} INFO - 24/12/12 11:13:41 INFO FileScanRDD: Reading File path: s3a://bw-airflow/lakehouse/bronze/order_item.parquet/part-00000-6d554dca-3697-45e1-bff0-a982f4054370-c000.snappy.parquet, range: 0-1644445, partition values: [empty row]
[2024-12-12T11:13:42.919-0300] {subprocess.py:93} INFO - 24/12/12 11:13:42 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.7.5:42939 in memory (size: 37.0 KiB, free: 434.3 MiB)
[2024-12-12T11:13:44.048-0300] {subprocess.py:93} INFO - 24/12/12 11:13:44 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:15:43.921-0300] {subprocess.py:93} INFO - 24/12/12 11:15:43 INFO FileOutputCommitter: Saved output of task 'attempt_202412121113343686980088891858646_0009_m_000000_16' to s3a://bw-airflow/lakehouse/silver/order_item.parquet/_temporary/0/task_202412121113343686980088891858646_0009_m_000000
[2024-12-12T11:15:44.131-0300] {subprocess.py:93} INFO - 24/12/12 11:15:43 INFO SparkHadoopMapRedUtil: attempt_202412121113343686980088891858646_0009_m_000000_16: Committed. Elapsed time: 6019 ms.
[2024-12-12T11:15:45.109-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO Executor: Finished task 0.0 in stage 9.0 (TID 16). 2821 bytes result sent to driver
[2024-12-12T11:15:45.359-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 16) in 129447 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:15:45.868-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-12-12T11:15:47.013-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO DAGScheduler: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0) finished in 130,093 s
[2024-12-12T11:15:48.527-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:15:48.828-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-12-12T11:15:48.837-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO DAGScheduler: Job 9 finished: parquet at NativeMethodAccessorImpl.java:0, took 130,097493 s
[2024-12-12T11:15:48.837-0300] {subprocess.py:93} INFO - 24/12/12 11:15:45 INFO FileFormatWriter: Start to commit write Job 7d5ff88f-a111-4372-a5b5-92106ceaaf27.
[2024-12-12T11:16:01.871-0300] {subprocess.py:93} INFO - 24/12/12 11:16:01 INFO FileFormatWriter: Write Job 7d5ff88f-a111-4372-a5b5-92106ceaaf27 committed. Elapsed time: 16766 ms.
[2024-12-12T11:16:01.908-0300] {subprocess.py:93} INFO - 24/12/12 11:16:01 INFO FileFormatWriter: Finished processing stats for write job 7d5ff88f-a111-4372-a5b5-92106ceaaf27.
[2024-12-12T11:16:04.892-0300] {subprocess.py:93} INFO - 24/12/12 11:16:04 INFO InMemoryFileIndex: It took 324 ms to list leaf files for 1 paths.
[2024-12-12T11:16:07.508-0300] {subprocess.py:93} INFO - 24/12/12 11:16:05 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:16:12.568-0300] {subprocess.py:93} INFO - 24/12/12 11:16:05 INFO DAGScheduler: Got job 10 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:16:14.881-0300] {subprocess.py:93} INFO - 24/12/12 11:16:05 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:16:15.131-0300] {subprocess.py:93} INFO - 24/12/12 11:16:05 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:16:15.147-0300] {subprocess.py:93} INFO - 24/12/12 11:16:05 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:16:15.147-0300] {subprocess.py:93} INFO - 24/12/12 11:16:07 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:16:15.147-0300] {subprocess.py:93} INFO - 24/12/12 11:16:07 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 107.3 KiB, free 433.7 MiB)
[2024-12-12T11:16:15.148-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)
[2024-12-12T11:16:15.148-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.7.5:42939 (size: 39.9 KiB, free: 434.2 MiB)
[2024-12-12T11:16:15.148-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:16:15.148-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:16:15.148-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-12-12T11:16:15.149-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 17) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 9864 bytes)
[2024-12-12T11:16:15.149-0300] {subprocess.py:93} INFO - 24/12/12 11:16:09 INFO Executor: Running task 0.0 in stage 10.0 (TID 17)
[2024-12-12T11:16:15.149-0300] {subprocess.py:93} INFO - 24/12/12 11:16:10 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.7.5:42939 in memory (size: 82.9 KiB, free: 434.3 MiB)
[2024-12-12T11:16:15.149-0300] {subprocess.py:93} INFO - 24/12/12 11:16:11 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:16:15.149-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO Executor: Finished task 0.0 in stage 10.0 (TID 17). 1889 bytes result sent to driver
[2024-12-12T11:16:15.150-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 17) in 2953 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:16:15.150-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-12-12T11:16:15.150-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO DAGScheduler: ResultStage 10 (parquet at NativeMethodAccessorImpl.java:0) finished in 5,337 s
[2024-12-12T11:16:15.150-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:16:15.150-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-12-12T11:16:15.150-0300] {subprocess.py:93} INFO - 24/12/12 11:16:12 INFO DAGScheduler: Job 10 finished: parquet at NativeMethodAccessorImpl.java:0, took 6,535510 s
[2024-12-12T11:16:15.151-0300] {subprocess.py:93} INFO - 24/12/12 11:16:15 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.7.5:42939 in memory (size: 37.8 KiB, free: 434.4 MiB)
[2024-12-12T11:16:16.655-0300] {subprocess.py:93} INFO - 24/12/12 11:16:16 INFO FileSourceStrategy: Pushed Filters:
[2024-12-12T11:16:17.299-0300] {subprocess.py:93} INFO - 24/12/12 11:16:16 INFO FileSourceStrategy: Post-Scan Filters:
[2024-12-12T11:16:17.541-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:16:18.521-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:16:18.955-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:16:25.699-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:17:00.354-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:17:17.699-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:17:18.409-0300] {subprocess.py:93} INFO - 24/12/12 11:16:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:17:18.410-0300] {subprocess.py:93} INFO - 24/12/12 11:16:57 INFO CodeGenerator: Code generated in 33640.746577 ms
[2024-12-12T11:17:18.410-0300] {subprocess.py:93} INFO - 24/12/12 11:16:57 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 219.5 KiB, free 434.0 MiB)
[2024-12-12T11:17:18.411-0300] {subprocess.py:93} INFO - 24/12/12 11:16:57 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 434.0 MiB)
[2024-12-12T11:17:18.411-0300] {subprocess.py:93} INFO - 24/12/12 11:16:57 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.7.5:42939 (size: 37.7 KiB, free: 434.3 MiB)
[2024-12-12T11:17:18.411-0300] {subprocess.py:93} INFO - 24/12/12 11:16:58 INFO SparkContext: Created broadcast 19 from parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:17:18.411-0300] {subprocess.py:93} INFO - 24/12/12 11:16:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:17:18.412-0300] {subprocess.py:93} INFO - 24/12/12 11:17:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:17:18.412-0300] {subprocess.py:93} INFO - 24/12/12 11:17:00 INFO DAGScheduler: Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:17:18.412-0300] {subprocess.py:93} INFO - 24/12/12 11:17:00 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:17:18.413-0300] {subprocess.py:93} INFO - 24/12/12 11:17:00 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:17:18.413-0300] {subprocess.py:93} INFO - 24/12/12 11:17:00 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:17:18.413-0300] {subprocess.py:93} INFO - 24/12/12 11:17:00 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[44] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:17:18.413-0300] {subprocess.py:93} INFO - 24/12/12 11:17:03 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 225.5 KiB, free 433.8 MiB)
[2024-12-12T11:17:18.414-0300] {subprocess.py:93} INFO - 24/12/12 11:17:17 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 82.4 KiB, free 433.7 MiB)
[2024-12-12T11:17:29.407-0300] {subprocess.py:93} INFO - 24/12/12 11:17:26 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.7.5:42939 (size: 82.4 KiB, free: 434.2 MiB)
[2024-12-12T11:17:33.065-0300] {subprocess.py:93} INFO - 24/12/12 11:17:32 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:17:35.525-0300] {subprocess.py:93} INFO - 24/12/12 11:17:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[44] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:17:35.571-0300] {subprocess.py:93} INFO - 24/12/12 11:17:32 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-12-12T11:17:35.572-0300] {subprocess.py:93} INFO - 24/12/12 11:17:32 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 18) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10598 bytes)
[2024-12-12T11:17:35.572-0300] {subprocess.py:93} INFO - 24/12/12 11:17:32 INFO Executor: Running task 0.0 in stage 11.0 (TID 18)
[2024-12-12T11:17:35.572-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:17:35.572-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:17:35.572-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO CodecConfig: Compression: SNAPPY
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-12-12T11:17:35.573-0300] {subprocess.py:93} INFO - 24/12/12 11:17:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO - {
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO -   "type" : "struct",
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO -   "fields" : [ {
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO -     "name" : "customer_id",
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:17:35.574-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:17:35.576-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:17:35.577-0300] {subprocess.py:93} INFO -     "name" : "date",
[2024-12-12T11:17:35.577-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:17:35.577-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:17:35.577-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:17:35.577-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:17:35.578-0300] {subprocess.py:93} INFO -     "name" : "id",
[2024-12-12T11:17:35.578-0300] {subprocess.py:93} INFO -     "type" : "long",
[2024-12-12T11:17:35.578-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:17:35.578-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:17:35.578-0300] {subprocess.py:93} INFO -   }, {
[2024-12-12T11:17:35.578-0300] {subprocess.py:93} INFO -     "name" : "status",
[2024-12-12T11:17:35.579-0300] {subprocess.py:93} INFO -     "type" : "string",
[2024-12-12T11:17:35.579-0300] {subprocess.py:93} INFO -     "nullable" : true,
[2024-12-12T11:17:35.579-0300] {subprocess.py:93} INFO -     "metadata" : { }
[2024-12-12T11:17:35.579-0300] {subprocess.py:93} INFO -   } ]
[2024-12-12T11:17:35.580-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:17:35.580-0300] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2024-12-12T11:17:35.580-0300] {subprocess.py:93} INFO - message spark_schema {
[2024-12-12T11:17:35.580-0300] {subprocess.py:93} INFO -   optional int64 customer_id;
[2024-12-12T11:17:35.580-0300] {subprocess.py:93} INFO -   optional binary date (STRING);
[2024-12-12T11:17:35.581-0300] {subprocess.py:93} INFO -   optional int64 id;
[2024-12-12T11:17:35.581-0300] {subprocess.py:93} INFO -   optional binary status (STRING);
[2024-12-12T11:17:35.581-0300] {subprocess.py:93} INFO - }
[2024-12-12T11:17:35.581-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:17:35.581-0300] {subprocess.py:93} INFO - 
[2024-12-12T11:17:38.999-0300] {subprocess.py:93} INFO - 24/12/12 11:17:38 INFO CodeGenerator: Code generated in 102.163979 ms
[2024-12-12T11:17:40.121-0300] {subprocess.py:93} INFO - 24/12/12 11:17:39 INFO FileScanRDD: Reading File path: s3a://bw-airflow/lakehouse/bronze/orders.parquet/part-00000-03cfe467-fdb1-480c-be8f-4e38f044e688-c000.snappy.parquet, range: 0-489945, partition values: [empty row]
[2024-12-12T11:17:41.905-0300] {subprocess.py:93} INFO - 24/12/12 11:17:40 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:18:07.003-0300] {subprocess.py:93} INFO - 24/12/12 11:18:07 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.7.5:42939 in memory (size: 39.9 KiB, free: 434.3 MiB)
[2024-12-12T11:18:19.671-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO FileOutputCommitter: Saved output of task 'attempt_202412121116591454412566688823241_0011_m_000000_18' to s3a://bw-airflow/lakehouse/silver/orders.parquet/_temporary/0/task_202412121116591454412566688823241_0011_m_000000
[2024-12-12T11:18:19.839-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO SparkHadoopMapRedUtil: attempt_202412121116591454412566688823241_0011_m_000000_18: Committed. Elapsed time: 5207 ms.
[2024-12-12T11:18:19.870-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO Executor: Finished task 0.0 in stage 11.0 (TID 18). 2821 bytes result sent to driver
[2024-12-12T11:18:19.873-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 18) in 47209 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:18:19.874-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-12-12T11:18:19.874-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO DAGScheduler: ResultStage 11 (parquet at NativeMethodAccessorImpl.java:0) finished in 79,020 s
[2024-12-12T11:18:19.874-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:18:19.874-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-12-12T11:18:19.875-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO DAGScheduler: Job 11 finished: parquet at NativeMethodAccessorImpl.java:0, took 79,024965 s
[2024-12-12T11:18:19.875-0300] {subprocess.py:93} INFO - 24/12/12 11:18:19 INFO FileFormatWriter: Start to commit write Job aa65c5c5-c422-4f31-b0fe-e6b4148593ea.
[2024-12-12T11:18:44.741-0300] {subprocess.py:93} INFO - 24/12/12 11:18:44 INFO FileFormatWriter: Write Job aa65c5c5-c422-4f31-b0fe-e6b4148593ea committed. Elapsed time: 24457 ms.
[2024-12-12T11:18:46.012-0300] {subprocess.py:93} INFO - 24/12/12 11:18:44 INFO FileFormatWriter: Finished processing stats for write job aa65c5c5-c422-4f31-b0fe-e6b4148593ea.
[2024-12-12T11:18:48.433-0300] {subprocess.py:93} INFO - 24/12/12 11:18:48 INFO InMemoryFileIndex: It took 278 ms to list leaf files for 1 paths.
[2024-12-12T11:18:52.312-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:18:52.554-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO DAGScheduler: Got job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:18:52.630-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:18:52.758-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:18:52.855-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:18:52.868-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[46] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:18:52.871-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 107.3 KiB, free 433.7 MiB)
[2024-12-12T11:18:52.871-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)
[2024-12-12T11:18:52.871-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.7.5:42939 (size: 39.9 KiB, free: 434.2 MiB)
[2024-12-12T11:18:52.872-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:18:52.872-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[46] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:18:52.872-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-12-12T11:18:52.872-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 19) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 9867 bytes)
[2024-12-12T11:18:52.873-0300] {subprocess.py:93} INFO - 24/12/12 11:18:52 INFO Executor: Running task 0.0 in stage 12.0 (TID 19)
[2024-12-12T11:18:53.291-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:18:53.852-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO Executor: Finished task 0.0 in stage 12.0 (TID 19). 2003 bytes result sent to driver
[2024-12-12T11:18:53.948-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 19) in 1046 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:18:54.080-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-12-12T11:18:54.282-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO DAGScheduler: ResultStage 12 (parquet at NativeMethodAccessorImpl.java:0) finished in 1,362 s
[2024-12-12T11:18:54.438-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:18:54.488-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-12-12T11:18:54.644-0300] {subprocess.py:93} INFO - 24/12/12 11:18:53 INFO DAGScheduler: Job 12 finished: parquet at NativeMethodAccessorImpl.java:0, took 1,688322 s
[2024-12-12T11:18:55.851-0300] {subprocess.py:93} INFO - 24/12/12 11:18:55 INFO InMemoryFileIndex: It took 269 ms to list leaf files for 1 paths.
[2024-12-12T11:18:56.279-0300] {subprocess.py:93} INFO - 24/12/12 11:18:56 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:18:56.369-0300] {subprocess.py:93} INFO - 24/12/12 11:18:56 INFO DAGScheduler: Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:18:56.581-0300] {subprocess.py:93} INFO - 24/12/12 11:18:56 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:19:01.617-0300] {subprocess.py:93} INFO - 24/12/12 11:18:56 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:19:03.059-0300] {subprocess.py:93} INFO - 24/12/12 11:18:56 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:19:03.073-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[48] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:19:03.073-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.7.5:42939 in memory (size: 39.9 KiB, free: 434.3 MiB)
[2024-12-12T11:19:03.073-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 107.3 KiB, free 433.7 MiB)
[2024-12-12T11:19:03.073-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)
[2024-12-12T11:19:03.074-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.7.5:42939 (size: 39.9 KiB, free: 434.2 MiB)
[2024-12-12T11:19:03.074-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:19:03.074-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[48] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:19:03.074-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-12-12T11:19:03.075-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 20) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 9868 bytes)
[2024-12-12T11:19:03.075-0300] {subprocess.py:93} INFO - 24/12/12 11:18:57 INFO Executor: Running task 0.0 in stage 13.0 (TID 20)
[2024-12-12T11:19:03.075-0300] {subprocess.py:93} INFO - 24/12/12 11:18:59 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:19:03.075-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO Executor: Finished task 0.0 in stage 13.0 (TID 20). 1912 bytes result sent to driver
[2024-12-12T11:19:03.076-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 20) in 4939 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:19:03.076-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-12-12T11:19:03.076-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO DAGScheduler: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 5,022 s
[2024-12-12T11:19:03.076-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:19:03.076-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-12-12T11:19:03.077-0300] {subprocess.py:93} INFO - 24/12/12 11:19:02 INFO DAGScheduler: Job 13 finished: parquet at NativeMethodAccessorImpl.java:0, took 6,048327 s
[2024-12-12T11:19:15.561-0300] {subprocess.py:93} INFO - 24/12/12 11:19:14 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.7.5:42939 in memory (size: 39.9 KiB, free: 434.3 MiB)
[2024-12-12T11:19:29.678-0300] {subprocess.py:93} INFO - 24/12/12 11:19:17 INFO InMemoryFileIndex: It took 4936 ms to list leaf files for 1 paths.
[2024-12-12T11:19:31.129-0300] {subprocess.py:93} INFO - 24/12/12 11:19:30 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-12-12T11:19:36.127-0300] {subprocess.py:93} INFO - 24/12/12 11:19:30 INFO DAGScheduler: Got job 14 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-12-12T11:19:38.605-0300] {subprocess.py:93} INFO - 24/12/12 11:19:30 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at NativeMethodAccessorImpl.java:0)
[2024-12-12T11:19:46.377-0300] {subprocess.py:93} INFO - 24/12/12 11:19:30 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:19:47.219-0300] {subprocess.py:93} INFO - 24/12/12 11:19:30 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:19:53.745-0300] {subprocess.py:93} INFO - 24/12/12 11:19:30 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[50] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-12-12T11:19:58.206-0300] {subprocess.py:93} INFO - 24/12/12 11:19:31 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 107.3 KiB, free 433.7 MiB)
[2024-12-12T11:19:58.886-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)
[2024-12-12T11:20:00.613-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.7.5:42939 (size: 39.9 KiB, free: 434.2 MiB)
[2024-12-12T11:20:03.760-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:20:05.312-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[50] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:20:05.591-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-12-12T11:20:07.532-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 21) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 9864 bytes)
[2024-12-12T11:20:13.837-0300] {subprocess.py:93} INFO - 24/12/12 11:19:32 INFO Executor: Running task 0.0 in stage 14.0 (TID 21)
[2024-12-12T11:20:14.627-0300] {subprocess.py:93} INFO - 24/12/12 11:19:38 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.7.5:42939 in memory (size: 82.4 KiB, free: 434.3 MiB)
[2024-12-12T11:20:16.275-0300] {subprocess.py:93} INFO - 24/12/12 11:19:38 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.7.5:42939 in memory (size: 37.7 KiB, free: 434.4 MiB)
[2024-12-12T11:20:16.366-0300] {subprocess.py:93} INFO - 24/12/12 11:19:43 INFO S3AInputStream: Switching to Random IO seek policy
[2024-12-12T11:20:16.425-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO Executor: Finished task 0.0 in stage 14.0 (TID 21). 1822 bytes result sent to driver
[2024-12-12T11:20:16.626-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 21) in 16091 ms on 192.168.7.5 (executor driver) (1/1)
[2024-12-12T11:20:16.713-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-12-12T11:20:23.142-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO DAGScheduler: ResultStage 14 (parquet at NativeMethodAccessorImpl.java:0) finished in 17,784 s
[2024-12-12T11:20:25.419-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-12-12T11:20:37.573-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-12-12T11:20:37.865-0300] {subprocess.py:93} INFO - 24/12/12 11:19:48 INFO DAGScheduler: Job 14 finished: parquet at NativeMethodAccessorImpl.java:0, took 18,181548 s
[2024-12-12T11:20:48.227-0300] {subprocess.py:93} INFO - 24/12/12 11:20:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customer_id),IsNotNull(id)
[2024-12-12T11:20:55.545-0300] {subprocess.py:93} INFO - 24/12/12 11:20:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customer_id#247L),isnotnull(id#249L)
[2024-12-12T11:22:21.453-0300] {subprocess.py:93} INFO - 24/12/12 11:20:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-12-12T11:22:22.813-0300] {subprocess.py:93} INFO - 24/12/12 11:20:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(id#201L)
[2024-12-12T11:22:22.989-0300] {subprocess.py:93} INFO - 24/12/12 11:20:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(order_id)
[2024-12-12T11:22:23.001-0300] {subprocess.py:93} INFO - 24/12/12 11:20:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(order_id#228L)
[2024-12-12T11:22:23.002-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO CodeGenerator: Code generated in 79684.011029 ms
[2024-12-12T11:22:23.002-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO CodeGenerator: Code generated in 76922.990254 ms
[2024-12-12T11:22:23.002-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 219.3 KiB, free 434.0 MiB)
[2024-12-12T11:22:23.003-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 219.3 KiB, free 433.8 MiB)
[2024-12-12T11:22:23.004-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 433.8 MiB)
[2024-12-12T11:22:23.004-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 433.8 MiB)
[2024-12-12T11:22:23.004-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 192.168.7.5:42939 (size: 37.7 KiB, free: 434.3 MiB)
[2024-12-12T11:22:23.005-0300] {subprocess.py:93} INFO - 24/12/12 11:22:22 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-12-12T11:22:23.005-0300] {subprocess.py:93} INFO - 24/12/12 11:22:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.7.5:42939 (size: 37.6 KiB, free: 434.3 MiB)
[2024-12-12T11:22:23.006-0300] {subprocess.py:93} INFO - 24/12/12 11:22:23 INFO SparkContext: Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-12-12T11:22:23.102-0300] {subprocess.py:93} INFO - 24/12/12 11:22:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:22:42.885-0300] {subprocess.py:93} INFO - 24/12/12 11:22:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-12-12T11:23:02.375-0300] {subprocess.py:93} INFO - 24/12/12 11:22:51 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 192.168.7.5:42939 in memory (size: 39.9 KiB, free: 434.3 MiB)
[2024-12-12T11:23:03.690-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-12-12T11:23:07.055-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2024-12-12T11:23:12.833-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Final stage: ResultStage 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2024-12-12T11:26:43.304-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:30:40.733-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:30:44.504-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[57] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2024-12-12T11:30:44.505-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-12-12T11:30:44.505-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.8 KiB, free 433.9 MiB)
[2024-12-12T11:30:44.505-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 433.9 MiB)
[2024-12-12T11:30:44.505-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.7.5:42939 (size: 6.6 KiB, free: 434.3 MiB)
[2024-12-12T11:30:44.506-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:30:44.506-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[57] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:30:44.506-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-12-12T11:30:44.506-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2024-12-12T11:30:44.506-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Final stage: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2024-12-12T11:30:44.507-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Parents of final stage: List()
[2024-12-12T11:30:44.507-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 22) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10372 bytes)
[2024-12-12T11:30:44.507-0300] {subprocess.py:93} INFO - 24/12/12 11:23:02 INFO DAGScheduler: Missing parents: List()
[2024-12-12T11:30:44.507-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[58] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2024-12-12T11:30:44.508-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 22)
[2024-12-12T11:30:44.508-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.6 KiB, free 433.9 MiB)
[2024-12-12T11:30:44.508-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 433.9 MiB)
[2024-12-12T11:30:44.508-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 192.168.7.5:42939 (size: 6.6 KiB, free: 434.3 MiB)
[2024-12-12T11:30:44.508-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2024-12-12T11:30:44.509-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[58] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2024-12-12T11:30:44.509-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-12-12T11:30:44.509-0300] {subprocess.py:93} INFO - 24/12/12 11:23:03 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 23) (192.168.7.5, executor driver, partition 0, PROCESS_LOCAL, 10373 bytes)
[2024-12-12T11:30:44.509-0300] {subprocess.py:93} INFO - 24/12/12 11:23:04 INFO Executor: Running task 0.0 in stage 16.0 (TID 23)
[2024-12-12T11:30:44.509-0300] {subprocess.py:93} INFO - 24/12/12 11:24:24 INFO CodeGenerator: Code generated in 72362.411374 ms
[2024-12-12T11:30:44.509-0300] {subprocess.py:93} INFO - 24/12/12 11:24:24 INFO CodeGenerator: Code generated in 72927.730841 ms
[2024-12-12T11:30:44.510-0300] {subprocess.py:93} INFO - 24/12/12 11:24:25 INFO FileScanRDD: Reading File path: s3a://bw-airflow/lakehouse/silver/order_item.parquet/part-00000-8ea84cbf-b4b3-449e-a9cf-fb893402fde9-c000.snappy.parquet, range: 0-1644247, partition values: [empty row]
[2024-12-12T11:30:44.510-0300] {subprocess.py:93} INFO - 24/12/12 11:24:28 INFO FileScanRDD: Reading File path: s3a://bw-airflow/lakehouse/silver/customers.parquet/part-00000-4d7269dd-b10f-41a0-9ac4-0b6a4171e0cb-c000.snappy.parquet, range: 0-252536, partition values: [empty row]
[2024-12-12T11:30:44.510-0300] {subprocess.py:93} INFO - 24/12/12 11:30:30 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray((22,15,0,Vector(AccumulableInfo(671,None,Some(495),None,false,true,None))), (23,16,0,Vector(AccumulableInfo(706,None,Some(495),None,false,true,None)))),Map((15,0) -> org.apache.spark.executor.ExecutorMetrics@65f7beb1, (16,0) -> org.apache.spark.executor.ExecutorMetrics@48132c00)) by listener SQLAppStatusListener took 3.474947394s.
[2024-12-12T11:30:43.950-0300] {local_task_job_runner.py:214} ERROR - Heartbeat time limit exceeded!
[2024-12-12T11:30:44.607-0300] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-12-12T11:30:44.616-0300] {process_utils.py:132} INFO - Sending 15 to group 72197. PIDs of all processes in the group: [72198, 72270, 72197]
[2024-12-12T11:30:44.616-0300] {process_utils.py:87} INFO - Sending the signal 15 to group 72197
[2024-12-12T11:30:44.616-0300] {taskinstance.py:2607} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-12-12T11:30:44.616-0300] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2024-12-12T11:30:44.933-0300] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-12-12T11:30:44.989-0300] {process_utils.py:80} INFO - Process psutil.Process(pid=72270, status='terminated', started='11:07:40') (72270) terminated with exit code None
[2024-12-12T11:30:48.576-0300] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/beatriz/Documentos/airflow/airflow_venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/beatriz/Documentos/airflow/airflow_venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/beatriz/Documentos/airflow/airflow_venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/beatriz/Documentos/airflow/airflow_venv/lib/python3.10/site-packages/airflow/operators/bash.py", line 234, in execute
    result = self.subprocess_hook.run_command(
  File "/home/beatriz/Documentos/airflow/airflow_venv/lib/python3.10/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/home/beatriz/Documentos/airflow/airflow_venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2609, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal
[2024-12-12T11:30:49.714-0300] {taskinstance.py:1205} INFO - Marking task as FAILED. dag_id=medallion_architecture, task_id=spark_submit, execution_date=20241212T140727, start_date=20241212T140737, end_date=20241212T143049
[2024-12-12T11:30:53.363-0300] {process_utils.py:80} INFO - Process psutil.Process(pid=72197, status='terminated', exitcode=2, started='11:07:37') (72197) terminated with exit code 2
[2024-12-12T11:30:53.778-0300] {process_utils.py:80} INFO - Process psutil.Process(pid=72198, status='terminated', started='11:07:37') (72198) terminated with exit code None
